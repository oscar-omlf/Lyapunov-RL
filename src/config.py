import numpy as np

from util.dynamics import (
    pendulum_dynamics_torch,
    pendulum_dynamics_np,
    pendulum_dynamics_dreal,
    compute_pendulum_reward,
    vanderpol_dynamics_torch,
    vanderpol_dynamics_dreal,
    bicycle_dynamics_torch,
    bicycle_dynamics_dreal
)

PENDULUM_G = 9.81
PENDULUM_M = 0.15
PENDULUM_L = 0.5
PENDULUM_B = 0.1
MAX_ACTION = 1.0


config_ldp_pendulum = {
    "agent_str": "LDP",
    "model_name": "LDP",
    "environment": "InvertedPendulum",
    "max_action": 1.0,
    "beta": 0.9,
    "dynamics_fn_dreal": pendulum_dynamics_dreal,
    "dynamics_fn": pendulum_dynamics_torch,
    "LQR": {
        "agent_str": "LQR",
        "environment": "InvertedPendulum",
        "discrete_discounted": False,
        "g": 9.81,
        "m": 0.15,
        "l": 0.5,
        "b": 0.1,
        "max_action": 1.0,
        "state_space": np.zeros(2),
        "action_space": np.zeros(1),
    },
    "alpha": 0.2,
    "lr": 2e-3,
    "batch_size": 128,
    "num_paths_sampled": 8,
    "norm_threshold": 5e-2,
    "integ_threshold": 500,
    "dt": 0.003,
    "actor_hidden_sizes":  (5, 5),
    "critic_hidden_sizes": (20, 20),
    "state_space": np.zeros(2),
    "action_space": np.zeros(1),
    "r1_bounds": (np.array([-2.0, -4.0]), np.array([2.0, 4.0])),
    "normalize_gradients": False,
    "c_star": 1.1523,
}

config_ldp_vanderpol = {
    "agent_str": "LDP",
    "model_name": "LDP",
    "environment": "VanDerPol",
    "max_action": 1.0,
    "beta": 0.8,
    "dynamics_fn_dreal": vanderpol_dynamics_dreal,
    "dynamics_fn": vanderpol_dynamics_torch,
    "LQR": {
        "agent_str": "LQR",
        "environment": "VanDerPol",
        "discrete_discounted": False,
        "mu": 1.0,
        "max_action": 1.0,
        "R": 0.1 * np.eye(1),
        "state_space": np.zeros(2),
        "action_space": np.zeros(1),
    },
    "alpha": 0.1,
    "lr": 3e-3,
    "batch_size": 128,
    "num_paths_sampled": 8,
    "norm_threshold": 5e-2,
    "integ_threshold": 150,
    "dt": 0.01,
    "actor_hidden_sizes":  (30, 30),
    "critic_hidden_sizes": (30, 30),
    "state_space": np.zeros(2),
    "action_space": np.zeros(1),
    "r1_bounds": (np.array([-2.0, -2.0]), np.array([2.0, 2.0])), 
    "normalize_gradients": True,
    "c_star": 3.9990,
}

config_lac_pendulum = {
    "agent_str": "Lyapunov-AC",
    "model_name": "LAC",
    "environment": "InvertedPendulum",
    "alpha": 0.2,
    "lr": 2e-3,
    "dynamics_fn": pendulum_dynamics_torch,
    "dynamics_fn_dreal": pendulum_dynamics_dreal,
    "batch_size": 128,
    "num_paths_sampled": 16,
    "dt": 0.003,
    "norm_threshold": 5e-2,
    "integ_threshold": 500,
    "r1_bounds": (np.array([-2.0, -4.0]), np.array([2.0, 4.0])),
    "actor_hidden_sizes": (5, 5), 
    "critic_hidden_sizes": (20, 20),
    "state_space": np.zeros(2), 
    "action_space": np.zeros(1), 
    "max_action": 1.0,
    "normalize_gradients": False
}

config_lac_vanderpol = {
    "agent_str": "Lyapunov-AC",
    "model_name": "LAC",
    "environment": "VanDerPol",
    "alpha": 0.1,
    "lr": 3e-3,
    "dynamics_fn": vanderpol_dynamics_torch,
    "dynamics_fn_dreal": vanderpol_dynamics_dreal,
    "batch_size": 128,
    "num_paths_sampled": 8,
    "dt": 0.01,
    "norm_threshold": 5e-2,
    "integ_threshold": 150,
    "r1_bounds": (np.array([-2.0, -2.0]), np.array([2.0, 2.0])),
    "actor_hidden_sizes": (30, 30), 
    "critic_hidden_sizes": (30, 30),
    "state_space": np.zeros(2), 
    "action_space": np.zeros(1), 
    "max_action": 1.0,
    "normalize_gradients": True
}

config_lac_bicycletracking = {
    "agent_str": "Lyapunov-AC",
    "model_name": "LAC",
    "environment": "BicycleTracking",
    "alpha": 1.5,
    "lr": 2e-3,
    "dynamics_fn": bicycle_dynamics_torch,
    "dynamics_fn_dreal": bicycle_dynamics_dreal,
    "batch_size": 128,
    "num_paths_sampled": 8,
    "dt": 0.003,
    "norm_threshold": 5e-2,
    "integ_threshold": 50,
    "r1_bounds": (np.array([-0.7, -0.7]), np.array([0.7, 0.7])),
    "actor_hidden_sizes": (20, 20), 
    "critic_hidden_sizes": (10, 10),
    "state_space": np.zeros(2), 
    "action_space": np.zeros(1), 
    "max_action": 1.0,
    "normalize_gradients": True
}


config_las_td3_pendulum = {
    "agent_str": "LAS_TD3",
    "model_name": "LAS_TD3",
    "environment": "InvertedPendulum",
    "max_action": MAX_ACTION,
    "beta": 0.6,
    "dynamics_fn_dreal": pendulum_dynamics_dreal,
    "LQR": {
        "agent_str": "LQR",
        "environment": "InvertedPendulum",
        "discrete_discounted": True,
        "gamma": 0.99,
        "dt": 0.003,
        "g": PENDULUM_G,
        "m": PENDULUM_M,
        "l": PENDULUM_L,
        "b": PENDULUM_B,
        "max_action": MAX_ACTION,
        "state_space": np.zeros(2),
        "action_space": np.zeros(1),
    },
    "gamma": 0.9,
    "tau": 0.005,
    "policy_freq": 2,
    "batch_size": 256,
    "policy_noise": 0.2,
    "noise_clip": 0.5,
    "expl_noise": 0.1,
    "actor_lr": 3e-4,
    "critic_lr": 3e-4,
    "actor_hidden_sizes": (256, 256),
    "critic_hidden_sizes": (256, 256),
    "state_space": np.zeros(2),
    "action_space": np.zeros(1),
    "r1_bounds": (np.array([-2.0, -4.0]), np.array([2.0, 4.0])),
    "c_star": 1.1982,
    }


config_td3_pendulum = {
    "agent_str": "TD3",
    "model_name": "TD3",
    "environment": "InvertedPendulum",
    "actor_lr": 3e-4,
    "critic_lr": 3e-4,
    "gamma": 0.99,
    "tau": 0.005,
    "batch_size": 256,
    "policy_freq": 2,
    "policy_noise": 0.2,
    "noise_clip": 0.5,
    "start_episodes": 100,
    "expl_noise": 0.1,
    "actor_hidden_sizes": (256, 256),
    "critic_hidden_sizes": (256, 256),
    "state_space": np.zeros(2),
    "action_space": np.zeros(1),
    "max_action": MAX_ACTION,
    "dynamics_fn": pendulum_dynamics_np,
    "rewards_fn": compute_pendulum_reward,
}

config_lqr_pendulum = {
    "agent_str": "LQR",
    "model_name": "LQR",
    "environment": "InvertedPendulum",
    "discrete_discounted": False,
    "gamma": 0.99,
    "dt": 0.003,
    "g": PENDULUM_G,
    "m": PENDULUM_M,
    "l": PENDULUM_L,
    "b": PENDULUM_B,
    "max_action": MAX_ACTION,
    "r1_bounds": (np.array([-2.0, -4.0]), np.array([2.0, 4.0])),
    "state_space": np.zeros(2),
    "action_space": np.zeros(1),
}